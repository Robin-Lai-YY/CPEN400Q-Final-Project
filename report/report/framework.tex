\section{Software}

We choose to use Pennylane for a few reasons:
\begin{itemize}
    \item Pennylane has excellent support for JAX \autocite{jax}, which investigated at the suggestion of Professor Olivia Di Matteo at our midterm check-in. JIT'ing the training step with JAX is several orders of magnitude faster than using PyTorch (we now see >1000 iterations per second).
    \item Pennylane is designed to be ``hardware and device agnostic'' \autocite{pennylane}. It offers many plugins that enable the same circuit written in Pennylane to be run on external quantum devices. Notably, we are able to run the quantum circuit on the JAX device (for low runtime), on the Qiskit Aer device (for simulating noise), and on IBMQ's real quantum hardware without rewriting the quantum circuit.
\end{itemize}

JAX also helped us solve a serious issue with machine learning demonstrations.
Since training is so sensitive to initial parameters, we want our data to be
completely reproducible.  Ordinarily, this can be solved by setting a seed
for the global random number generator (for example, with Numpy).  However,
this is not ideal.  Say we generate the training examples by invoking the RNG,
and then generate the initial parameters by invoking it again.  It is 
repeatable, but interferes with experimentation.  If we were to increase the 
number of generated training examples, it would completely change the state of
the RNG when it is used to generate the initial parameters, influencing the 
outcome of training.

We use JAX's splittable RNG keys in a way that tries to minimize this 
unintended effect: to set up a GAN the user must pass a key, which is split
into the subkeys required for each task.  How the randomness is used after the
key is split has no effect on the other keys.

We factored out the most reusable components into a library, the 
\texttt{quantumgan} module, in order to make experiments with quantum GANs
as easy as possible.  The code to train GANs (which is rather more involved
than for many other machine learning models, since we need to interleave two
optimizers and take multiple gradients with respect to some parameters but
not all) is factored out into its own module, \texttt{quantumgan.train},
which will train any model that implements our \texttt{GAN} interface. Both
our classical GAN and batch GAN implement this interface, and so are
trained with the same code.

While one benefit of the batch GAN is to be able to train with many training
examples at once, the paper never does any simulations with more than zero
index qubits (making them completely sequential).  We added support for this
and tested it (simply setting the \texttt{batch\_size} greater than zero in the arguments to
\texttt{BatchGAN} will set up the index bits necessary).

The paper also mentions that MPQCs are trainable in many situations, even if
the rotations and entanglers used are not the ones given.  The layout of the entanglers can also be varied to better suit the available hardware.  The paper, and our
implementation, default to $CZ$ and $RY$, but it is readily configured or extended by implementing the \texttt{MPQC} interface:

\begin{minted}[fontsize=\footnotesize]{python}
features_dim, batch_size = 4, 2
gen_params, dis_params = BatchGAN.init_params(
    params_key, features_dim,
    gen_layers=3, gen_ancillary=1, 
    dis_layers=3, dis_ancillary=1,
)
gan = BatchGAN(
    features_dim, batch_size, gen_params, dis_params,
    trainable=qml.RZ,
    entangler=RandomEntangler(key, entangler=qml.CNOT),
)
\end{minted}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{plots/index_cnot_circuit.pdf}
    \caption{An exotic batch GAN produced by the above code, containing an index register and a pseudorandom entangler layout.}
    \label{fig:index_circuit}
\end{figure}

