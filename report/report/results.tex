\section{Results}
Huang et al. demonstrate the patch architecture with an $8\times 8$ pixel, resized
MNIST digit dataset.  This is practical to run when divided into four patches of
size $8\times 2$, but a patch strategy does not apply for the discriminator (it must
be large enough to ``see'' the generated image).  To demonstrate batch GANs,
they construct an artificial dataset small enough that an entire training
example can be embedded into the feature register at once.

The dataset, referred to as the ``grayscale bar dataset'' in the paper, is
defined by giving an explicit probability distribution for a random image
$\bm{x}_{ij}$ (where $i$ is the row and $j$ is the column in the graphical
representation).  Some typographic errors from the paper have been corrected:

\begin{align*}
  \bm{x}_{00} &\sim \text{unif}(0.4, 0.6) \\
  \bm{x}_{10} &= 1 - \bm{x}_{00} \\
  \bm{x}_{i1} &= 0 \\
\end{align*}

The distribution is already normalized so that it sums to $1$ (a prerequisite
for the quantum generator).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{plots/bar_data.pdf}
    \caption{Samples from our generator for the bar dataset.}
    \label{fig:bar_data}
\end{figure}

The authors then train a batch GAN with 12 trainable parameters,
including a $3$-layer quantum generator with $1$ ancillary qubit, and
a $4$-layer quantum discriminator with $1$ ancillary qubit.

Both the generator and discriminator use the same multi-layer parameterized circuit: 
$RY$ trainable gates and $CZ$ in the ``staircase'' configuration for the entanglers.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{plots/batch_gan_circuit.pdf}
    \caption{Our best guess at the exact quantum batch circuit used in the paper.}
    \label{fig:batch_gan_circuit}
\end{figure}

Figure \ref{fig:batch_gan_circuit} shows the circuit generated by our software 
(drawn by Pennylane).   The generator ancillary qubits are labelled ``agn'', 
discriminator ancillary ``adn'', and the feature register is labelled ``fn''.
If there were index qubits, they would be labelled ``in''.

After training the batch GAN for 350 iterations, its performance is compared with a classical 
GAN with a similar number of trainable parameters.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{plots/fd_scores.pdf}
    \caption{The performance of quantum batch GAN on idea device, quantum batch GAN on noisy device, classical GAN with MLP generator, and classical GAN with CNN generator.}
    \label{fig:fd_scores}
\end{figure}

In Figure \ref{fig:fd_scores}, we compare the performance of our batch GAN, and the classical GANs 
described in the paper. All of them have similar number of trainable parameters ($\approx$11), but the best batch GANs show 
a much lower minimum FD score (more closely resembles the dataset).  We found that, like the paper,
quantum GANs are much more variable with respect to their initial parameters.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{plots/generated_bars.pdf}
    \caption{Some generated images from one particular trained batch GAN.}
    \label{fig:generated_bars}
\end{figure}

For example, in Figure \ref{fig:loss_compare}, the same 12-parameter batch GAN is trained
with differing initial parameters.  The left GAN converges and generates good images in just under 
1500 iterations, while the right has still not converged after 2000.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{plots/loss_compare.pdf}
    \caption{Comparison of the loss over time when training starting with two sets of random initial parameters.}
    \label{fig:loss_compare}
\end{figure}

Additionally, we compare quantum batch GANs with varying numbers of index qubits. 
The results are illustrated in Figure~\ref{fig:loss_compare_index}, which presents the loss over time with different number of index qubits.
We observe that increasing the number of index qubits can indeed leverage parallelism and 
lower the number of iterations required for convergence. Note that the original paper does not
include similar comparison; it does not present any evaluation with more than 0 index qubit.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{plots/loss_compare_index.pdf}
    \caption{Comparison of the loss over time when training starting with different sets of initial parameters and numbers of index qubits.}
    \label{fig:loss_compare_index}
\end{figure}
