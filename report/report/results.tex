\section{Results}
We focused on reproducing the results for the ``batch'' strategy only---an
implementation of quantum patch GANs is available online
\autocite{ellis2022quantum}.

Huang et al. demonstrate the patch architecture with an $8\times 8$ pixel, resized
MNIST digit dataset.  This is practical to run when divided into four patches of
size $8\times 2$, but a patch strategy does not apply for the discriminator (it must
be large enough to ``see'' the generated image).  To demonstrate batch GANs,
they construct an artificial dataset small enough that an entire training
example can be embedded into the feature register at once.

The dataset, referred to as the ``grayscale bar dataset'' in the paper, is
defined by giving an explicit probability distribution for a random image
$\bm{x}_{ij}$ (where $i$ is the row and $j$ is the column in the graphical
representation).  Some typographic errors from the paper have been corrected:

\begin{align*}
  \bm{x}_{00} &\sim \text{unif}(0.4, 0.6) \\
  \bm{x}_{10} &= 1 - \bm{x}_{00} \\
  \bm{x}_{i1} &= 0 \\
\end{align*}

The distribution is already normalized so that it sums to $1$ (a prerequisite
for the quantum generator).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{plots/bar_data.pdf}
    \caption{Samples from our generator for the bar dataset.}
    \label{fig:bar_data}
\end{figure}

The authors then train a batch GAN with 12 trainable parameters,
including a $3$-layer quantum generator with $1$ ancillary qubit, and
a $4$-layer quantum discriminator with $1$ ancillary qubit.

Both the generator and discriminator used the same multi-layer parameterized circuit: 
$RY$ trainable gates and $CZ$ in the ``staircase'' configuration for the entanglers.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{plots/batch_gan_circuit.pdf}
    \caption{Our best guess at the exact quantum batch circuit used in the paper.}
    \label{fig:batch_gan_circuit}
\end{figure}

Figure \ref{fig:batch_gan_circuit} shows the circuit generated by our software 
(drawn by Pennylane).   The generator ancillary qubits are labelled ``agn'', 
discriminator ancillary as ``adn'', and the feature register is labelled ``fn''.
If there were index qubits, they would be labelled ``in'' (more on this later).

After training the batch GAN for 350 iterations, its performance is compared with a classical 
GAN with a similar number of trainable parameters.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{plots/fd_scores.pdf}
    \caption{The performance of the quantum batch GAN vs. the classical GAN with MLP generator.}
    \label{fig:fd_scores}
\end{figure}

In \ref{fig:fd_scores}, we compare the performance of our batch GAN, and the multi-layer
perceptron described in the paper.  Both have 9 trainable parameters, but the best batch GANs show 
a much lower minimum FD score (more closely resembles the dataset).  We found that, like the paper,
quantum GANs are much more variable with respect to their initial parameters.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{plots/generated_bars.pdf}
    \caption{Some generated images from one trained batch GAN.}
    \label{fig:generated_bars}
\end{figure}
